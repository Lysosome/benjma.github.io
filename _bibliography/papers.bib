@inproceedings{greer2019multimodal,
  title={A multimodal view into music's effect on human neural, physiological, and emotional experience},
  author={Greer, Timothy and Ma, Benjamin and Sachs, Matthew and Habibi, Assal and Narayanan, Shrikanth},
  booktitle={Proceedings of the 27th ACM international conference on multimedia},
  pages={167--175},
  year={2019},
  abbr={ACM}
}

@inproceedings{ma2019predicting,
  title={Predicting human-reported enjoyment responses in happy and sad music},
  author={Ma, Benjamin and Greer, Timothy and Sachs, Matthew and Habibi, Assal and Kaplan, Jonas and Narayanan, Shrikanth},
  booktitle={2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)},
  pages={607--613},
  year={2019},
  organization={IEEE},
  selected={true},
  abbr={ACII}
}

@article{ma2021computational,
  title={A computational lens into how music characterizes genre in film},
  author={Ma, Benjamin and Greer, Timothy and Knox, Dillon and Narayanan, Shrikanth},
  journal={PloS one},
  volume={16},
  number={4},
  pages={e0249957},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA},
  selected={true},
  abbr={PLoS ONE}
}

@inproceedings{greer2019learning,
  title={Learning shared vector representations of lyrics and chords in music},
  author={Greer, Timothy and Singla, Karan and Ma, Benjamin and Narayanan, Shrikanth},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3951--3955},
  year={2019},
  organization={IEEE},
  abbr={ICASSP}
}

@inproceedings{knox2021loss,
  title={Loss Function Approaches for Multi-label Music Tagging},
  author={Knox, Dillon and Greer, Timothy and Ma, Benjamin and Kuo, Emily and Somandepalli, Krishna and Narayanan, Shrikanth},
  booktitle={2021 International Conference on Content-Based Multimedia Indexing (CBMI)},
  pages={1--4},
  year={2021},
  organization={IEEE},
  abbr={CBMI}
}

@article{flemotomos2021coordination,
  title={Coordination or Dominance? An Investigation of Social Dynamics in Conversational Entrainment},
  author={Flemotomos, Nikolaos and Ma, Benjamin and Peri, Raghuveer},
  year={2021},
  abbr={USC}
}

@inproceedings{knox2020mediaeval,
  title={MediaEval 2020 Emotion and Theme Recognition in Music Task: Loss Function Approaches for Multi-label Music Tagging.},
  author={Knox, Dillon and Greer, Timothy and Ma, Benjamin and Kuo, Emily and Somandepalli, Krishna and Narayanan, Shrikanth},
  booktitle={MediaEval},
  year={2020},
  abbr={MediaEval}
}

@article{greer2022multi,
  title={Multi-modal, Multi-task, Music BERT: A Context-Aware Music Encoder Based on Transformers},
  author={Greer, Timothy and Shi, Xuan and Ma, Benjamin and Narayanan, Shrikanth},
  year={2022},
  abbr={USC}
}

ï»¿@Article{Greer2023,
author={Greer, Timothy
and Shi, Xuan
and Ma, Benjamin
and Narayanan, Shrikanth},
title={Creating musical features using multi-faceted, multi-task encoders based on transformers},
journal={Scientific Reports},
year={2023},
month={Jul},
day={03},
volume={13},
number={1},
pages={10713},
abstract={Computational machine intelligence approaches have enabled a variety of music-centric technologies in support of creating, sharing and interacting with music content. A strong performance on specific downstream application tasks, such as music genre detection and music emotion recognition, is paramount to ensuring broad capabilities for computational music understanding and Music Information Retrieval. Traditional approaches have relied on supervised learning to train models to support these music-related tasks. However, such approaches require copious annotated data and still may only provide insight into one view of music---namely, that related to the specific task at hand. We present a new model for generating audio-musical features that support music understanding, leveraging self-supervision and cross-domain learning. After pre-training using masked reconstruction of musical input features using self-attention bidirectional transformers, output representations are fine-tuned using several downstream music understanding tasks. Results show that the features generated by our multi-faceted, multi-task, music transformer model, which we call M3BERT, tend to outperform other audio and music embeddings on several diverse music-related tasks, indicating the potential of self-supervised and semi-supervised learning approaches toward a more generalized and robust computational approach to modeling music. Our work can offer a starting point for many music-related modeling tasks, with potential applications in learning deep representations and enabling robust technology applications.},
issn={2045-2322},
doi={10.1038/s41598-023-36714-z},
url={https://doi.org/10.1038/s41598-023-36714-z},
abbr={Sci. Rep.}
}

